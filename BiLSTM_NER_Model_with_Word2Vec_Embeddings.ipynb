{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation Pipeline for DataTurks .tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from seqeval.metrics import f1_score,classification_report\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional,Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Data to Sequences\n",
    "\n",
    "### Contents:\n",
    "\n",
    "1. Reading File and formatting as sequences\n",
    "2. Formatting Entities to IOB Scheme\n",
    "3. Padding Sequences\n",
    "4. Mapping to Integer Ids\n",
    "5. Formatting Data for Keras LSTM Model\n",
    "6. Train Test Split\n",
    "7. Specifying Model and Model Parameters\n",
    "8. Training Model\n",
    "9. Evaluating Model\n",
    "10. Saving Model Results\n",
    "\n",
    "### 1. Reading and Formatting File:\n",
    "\n",
    "The file being used is the raw output of a data turks annotated tsv file.\n",
    "\n",
    "More info Available:\n",
    "https://dataturks.com/features/document-ner-annotation.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_turks(file):\n",
    "    with open(file) as f:\n",
    "        lines = [i.rstrip().split(\"\\t\") for i in f.readlines()]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\"In', 'O'],\n",
       " ['the', 'O'],\n",
       " ['Hospital3', 'GEO'],\n",
       " ['Emergency', 'GEO'],\n",
       " ['Room', 'GEO'],\n",
       " [',', 'O'],\n",
       " ['her', 'O'],\n",
       " ['oxygen', 'O'],\n",
       " ['saturation', 'O'],\n",
       " ['was', 'O'],\n",
       " ['100%', 'O2 Saturation'],\n",
       " ['on', 'O2 Saturation'],\n",
       " ['CPAP', 'O2 Saturation'],\n",
       " ['\"', 'O'],\n",
       " [''],\n",
       " ['She', 'O'],\n",
       " ['was', 'O'],\n",
       " ['not', 'DOS'],\n",
       " ['able', 'DOS'],\n",
       " ['to', 'DOS']]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"./data/Medical NER V2 2000.tsv\"\n",
    "word_ents = read_turks(file)\n",
    "word_ents[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words (such as the first above) contained a number and quote before it so these are removed with the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(word_ents):\n",
    "    '''removes quote and comma characters from'''\n",
    "    new_word_ents = []\n",
    "    for ents in word_ents:\n",
    "        word = ents[0].lower()\n",
    "        word = word.replace('\"','')\n",
    "        ents[0] = word\n",
    "        new_word_ents.append(ents)\n",
    "    return new_word_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['in', 'O'],\n",
       " ['the', 'O'],\n",
       " ['hospital3', 'GEO'],\n",
       " ['emergency', 'GEO'],\n",
       " ['room', 'GEO'],\n",
       " [',', 'O'],\n",
       " ['her', 'O'],\n",
       " ['oxygen', 'O'],\n",
       " ['saturation', 'O'],\n",
       " ['was', 'O'],\n",
       " ['100%', 'O2 Saturation'],\n",
       " ['on', 'O2 Saturation'],\n",
       " ['cpap', 'O2 Saturation'],\n",
       " ['', 'O'],\n",
       " [''],\n",
       " ['she', 'O'],\n",
       " ['was', 'O'],\n",
       " ['not', 'DOS'],\n",
       " ['able', 'DOS'],\n",
       " ['to', 'DOS']]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ents = clean_words(word_ents)\n",
    "new_ents[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataturks uses a blank line to seperate each sequence. This is why most csv/tsv readers cannot read the file. The following function will split each sequence when it finds a blank line in the tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seqs(word_ents):\n",
    "    seqs = []\n",
    "    seq = []\n",
    "    for ents in word_ents:\n",
    "        if len(ents)>1:\n",
    "            if len(ents[0])>0:\n",
    "                if ents[0][-1] == \".\":\n",
    "                    seq.append([ents[0][:-1],ents[1]])\n",
    "                else:\n",
    "                    seq.append(ents)\n",
    "        else:\n",
    "            seqs.append(seq)\n",
    "            seq=[]\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['in', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['hospital3', 'GEO'],\n",
       "  ['emergency', 'GEO'],\n",
       "  ['room', 'GEO'],\n",
       "  [',', 'O'],\n",
       "  ['her', 'O'],\n",
       "  ['oxygen', 'O'],\n",
       "  ['saturation', 'O'],\n",
       "  ['was', 'O'],\n",
       "  ['100%', 'O2 Saturation'],\n",
       "  ['on', 'O2 Saturation'],\n",
       "  ['cpap', 'O2 Saturation']]]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = create_seqs(new_ents)\n",
    "seqs[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_word(ent):\n",
    "    '''Splits at specified special characters keeping special characters as their own value'''\n",
    "    words = re.split('([/\\-\\%><])',ent[0])\n",
    "    return [[i,ent[1]] for i in words if len(i)>0]\n",
    "\n",
    "def expand_special_chars(seqs):\n",
    "    '''Expands special characters in words into seperate words while '''\n",
    "    new_seqs = []\n",
    "    for seq in seqs:\n",
    "        new_seq = []\n",
    "        for word in seq:\n",
    "            new_seq += expand_word(word)\n",
    "        new_seqs.append(new_seq)\n",
    "    return new_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['in', 'O'],\n",
       " ['the', 'O'],\n",
       " ['hospital3', 'GEO'],\n",
       " ['emergency', 'GEO'],\n",
       " ['room', 'GEO'],\n",
       " [',', 'O'],\n",
       " ['her', 'O'],\n",
       " ['oxygen', 'O'],\n",
       " ['saturation', 'O'],\n",
       " ['was', 'O'],\n",
       " ['100', 'O2 Saturation'],\n",
       " ['%', 'O2 Saturation'],\n",
       " ['on', 'O2 Saturation'],\n",
       " ['cpap', 'O2 Saturation']]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_seqs = expand_special_chars(seqs)\n",
    "ex_seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_numerics(seq):\n",
    "    '''Add encodings for common number types'''\n",
    "    enc_seq = []\n",
    "    for ent in seq:\n",
    "        enc = ent[0].strip()\n",
    "        if re.match(\"^\\d$\",ent[0]) != None:\n",
    "            enc = \"<1DigitNum>\"\n",
    "        elif re.match(\"^\\d\\d$\",ent[0]) != None:\n",
    "            enc = \"<2DigitNum>\"\n",
    "        elif re.match(\"^\\d\\d\\d$\",ent[0]) !=None:\n",
    "            enc = \"<3DigitNum>\"\n",
    "        elif re.match(\"^\\d{4}$\",ent[0]) != None:\n",
    "            enc = \"4DigitNum\"\n",
    "        elif re.match(\"^\\d*\\.\\d*$\",ent[0]) != None:\n",
    "            enc = \"<DecimalNum>\"\n",
    "        elif re.match(\"^\\d+,\\d+$\",ent[0]) != None:\n",
    "            enc = \"<CommaNum>\"\n",
    "        elif re.match(\"^\\d+'?s$\",ent[0]) !=None:\n",
    "            enc = \"<RangeNum>\"\n",
    "            \n",
    "        enc_seq.append([enc,ent[1]])\n",
    "    return enc_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['in', 'O'],\n",
       " ['the', 'O'],\n",
       " ['hospital3', 'GEO'],\n",
       " ['emergency', 'GEO'],\n",
       " ['room', 'GEO'],\n",
       " [',', 'O'],\n",
       " ['her', 'O'],\n",
       " ['oxygen', 'O'],\n",
       " ['saturation', 'O'],\n",
       " ['was', 'O'],\n",
       " ['<3DigitNum>', 'O2 Saturation'],\n",
       " ['%', 'O2 Saturation'],\n",
       " ['on', 'O2 Saturation'],\n",
       " ['cpap', 'O2 Saturation']]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_seqs = [encode_numerics(seq) for seq in ex_seqs]\n",
    "enc_seqs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Formatting Entities to IOB (Inside,Outside, Beginning) Scheme \n",
    "\n",
    "This scheme adds more context to the tags and allows annotations to make more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tags(word_ents):\n",
    "    '''adds IOB scheme to tags'''\n",
    "    new_ents = []\n",
    "    for i in range(0,len(word_ents)):\n",
    "        if word_ents[i][1] == \"O\":\n",
    "            tag = word_ents[i][1]\n",
    "        else:\n",
    "            if not i:\n",
    "                tag = \"B-\"+word_ents[i][1]\n",
    "            else:\n",
    "                if (word_ents[i][1] != word_ents[i-1][1]):\n",
    "                    tag = \"B-\"+word_ents[i][1]\n",
    "                else:\n",
    "                    tag = \"I-\"+word_ents[i][1]\n",
    "\n",
    "        new_ents.append([word_ents[i][0],tag])\n",
    "    return new_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['in', 'O'],\n",
       " ['the', 'O'],\n",
       " ['hospital3', 'B-GEO'],\n",
       " ['emergency', 'I-GEO'],\n",
       " ['room', 'I-GEO'],\n",
       " [',', 'O'],\n",
       " ['her', 'O'],\n",
       " ['oxygen', 'O'],\n",
       " ['saturation', 'O'],\n",
       " ['was', 'O'],\n",
       " ['<3DigitNum>', 'B-O2 Saturation'],\n",
       " ['%', 'I-O2 Saturation'],\n",
       " ['on', 'I-O2 Saturation'],\n",
       " ['cpap', 'I-O2 Saturation']]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tag_seqs = [clean_tags(ents) for ents in enc_seqs]\n",
    "cleaned_tag_seqs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Padding Sequences to a Specified Length\n",
    "\n",
    "In order to be usable by the LSTM model, each sequence needs to be padded/truncated to the same length. Here 50 is chosen somewhat arbitraily but is around the 97th percentile of sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(seq,max_len):\n",
    "    padded_seq = seq+[[\"<PAD>\",\"O\"]]*max_len\n",
    "    return padded_seq[:max_len]\n",
    "    \n",
    "def pad_sequences(sequences,max_len=None):\n",
    "    if max_len == None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "    return [pad_seq(seq,max_len) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['in', 'O'],\n",
       " ['the', 'O'],\n",
       " ['hospital3', 'GEO'],\n",
       " ['emergency', 'GEO'],\n",
       " ['room', 'GEO'],\n",
       " [',', 'O'],\n",
       " ['her', 'O'],\n",
       " ['oxygen', 'O'],\n",
       " ['saturation', 'O'],\n",
       " ['was', 'O'],\n",
       " ['<3DigitNum>', 'O2 Saturation'],\n",
       " ['%', 'O2 Saturation'],\n",
       " ['on', 'O2 Saturation'],\n",
       " ['cpap', 'O2 Saturation'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O']]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 50\n",
    "padded_seqs = pad_sequences(enc_seqs,max_len)\n",
    "padded_seqs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Mapping Words to Integer Values for Model Training\n",
    "\n",
    "The model can't use words so each one is mapped to a particular index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ids(sentances,tag=False):\n",
    "    words = []\n",
    "    for sentance in sentances:\n",
    "        words += list([word[tag] for word in sentance])\n",
    "    word_dict = {word:i for i,word in enumerate(set(words))}\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bilious', 0), ('apical', 1), ('embolism', 2), ('suggested', 3)]\n",
      "[('Date', 0), ('Temperature', 1), ('Drug', 2), ('Condition', 3)]\n"
     ]
    }
   ],
   "source": [
    "word_ids = get_word_ids(padded_seqs)\n",
    "tag_ids = get_word_ids(padded_seqs,tag=True)\n",
    "print(list(word_ids.items())[:4])\n",
    "print(list(tag_ids.items())[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_ids(sentances,word_ids,tag_ids):\n",
    "    vector = []\n",
    "    for sentance in sentances:\n",
    "        vector.append(list([[word_ids[w[0]],tag_ids[w[1]]] for w in sentance]))\n",
    "    return np.array(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the words are given a numeric representation which can be mapped back to the original words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2485   18]\n",
      " [2448   18]\n",
      " [3605   19]\n",
      " [1476   19]]\n",
      "\n",
      "Word Representation:\n",
      "[['in', 'O'], ['the', 'O'], ['hospital3', 'GEO'], ['emergency', 'GEO']]\n"
     ]
    }
   ],
   "source": [
    "vectors = words_to_ids(padded_seqs,word_ids,tag_ids)\n",
    "print(vectors[0][:4])\n",
    "print('')\n",
    "print(\"Word Representation:\")\n",
    "print(padded_seqs[0][:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can label our features (x) and labels (y) for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x_y(matrix,n_tags):\n",
    "    x = []\n",
    "    y = []\n",
    "    for sequences in matrix:\n",
    "        xi = [i[0] for i in sequences]\n",
    "        yi = [i[1] for i in sequences]\n",
    "        x.append(xi)\n",
    "        y.append(yi)\n",
    "    y = np.array([to_categorical(i,n_tags) for i in y])\n",
    "    return np.array(x),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-shape: (2009, 50)\n",
      "[2485 2448 3605 1476 1109]\n",
      "\n",
      "Y-shape: (2009, 50, 26)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "n_tags = len(tag_ids)\n",
    "x,y = create_x_y(vectors,n_tags)\n",
    "print(\"X-shape:\",x.shape)\n",
    "print(x[0][:5])\n",
    "print('')\n",
    "print(\"Y-shape:\",y.shape)\n",
    "print(y[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Specifying Model and Model Parameters\n",
    "\n",
    "https://www.depends-on-the-definition.com/introduction-named-entity-recognition-python/\n",
    "\n",
    "#### Use Word2Vec Embedding Trained on Entire Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/LiamRoberts/anaconda3/envs/keras36/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "/Users/LiamRoberts/anaconda3/envs/keras36/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "def create_weight_matrix(word_ids,embeddings_index):\n",
    "    embedding_matrix = np.zeros((len(word_ids),100))\n",
    "    count = 0\n",
    "    oov_words = []\n",
    "    for word,idx in word_ids.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[idx] = embeddings_index[word]\n",
    "        else:\n",
    "            if word == \"<PAD>\":\n",
    "                embedding_matrix[idx] = np.array([999]*100)\n",
    "            else:\n",
    "                oov_words.append(word)\n",
    "    return embedding_matrix,oov_words    \n",
    "\n",
    "custom_emb = Word2Vec.load(\"./data/word2vec_numeric_encs.model\")\n",
    "embed_matrix,oov_words = create_weight_matrix(word_ids,custom_emb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent OOV: 0.2959309494451295%\n",
      "['t95', 'hbc', 'oes', 'letharagic', 'hyperlipidema', 'echopraxia', '1130hrs', 't=101', 'bedsid', 'tasty', '4.75l', \"plt's\"]\n"
     ]
    }
   ],
   "source": [
    "# OOV Words\n",
    "print(f\"Percent OOV: {len(oov_words)/len(word_ids)*100}%\")\n",
    "print(oov_words[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of how some words are split there are still some words out of vocabulary even though the Word2Vec embeddings were trained on the full  version of the same corpus. Each of these OOV words will simply have 0 weights in the matrix. The 0.9% of OOV words is a huge improvement over what was seen using Glove embeddings which saw 13% of the vocab being OOV words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_BiLSTM(n_words,n_tags,embedding_size,max_len,embed_weights):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(n_words,\n",
    "                        embedding_size,\n",
    "                        weights=[embed_weights],\n",
    "                        trainable=False,\n",
    "                        input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.2)))\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"softmax\")))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tester():\n",
    "    return 1,2,3,4\n",
    "a,b,c,d = tester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 50, 100)           405500    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 50, 200)           160800    \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 50, 26)            5226      \n",
      "=================================================================\n",
      "Total params: 571,526\n",
      "Trainable params: 166,026\n",
      "Non-trainable params: 405,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 100\n",
    "n_words = len(word_ids)\n",
    "n_tags = len(tag_ids)\n",
    "\n",
    "model = create_BiLSTM(n_words,n_tags,embedding_size,max_len,embed_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model(model,x_train,y_train,batch_size=32,epochs=20,val_split = 0.1):\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0.0001,\n",
    "                               patience=0,\n",
    "                               mode='min',\n",
    "                               verbose=1)\n",
    "    \n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit(x_train, y_train, \n",
    "                        batch_size=32, \n",
    "                        epochs=epochs, \n",
    "                        validation_split=val_split, \n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stop]\n",
    "                       )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1808 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1a4537fea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1a4537fea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.5728 - accuracy: 0.8521WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 11s 6ms/sample - loss: 0.5707 - accuracy: 0.8527\n",
      "Epoch 2/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.9155WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 8s 5ms/sample - loss: 0.2923 - accuracy: 0.9153\n",
      "Epoch 3/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.2259 - accuracy: 0.9321WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 9s 5ms/sample - loss: 0.2268 - accuracy: 0.9318\n",
      "Epoch 4/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.1982 - accuracy: 0.9386WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 8s 5ms/sample - loss: 0.1981 - accuracy: 0.9386\n",
      "Epoch 5/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.1793 - accuracy: 0.9432WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 9s 5ms/sample - loss: 0.1795 - accuracy: 0.9432\n",
      "Epoch 6/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.1645 - accuracy: 0.9477WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 8s 5ms/sample - loss: 0.1644 - accuracy: 0.9477\n",
      "Epoch 7/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.1528 - accuracy: 0.9506WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 8s 4ms/sample - loss: 0.1531 - accuracy: 0.9505\n",
      "Epoch 8/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.1437 - accuracy: 0.9544WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 8s 4ms/sample - loss: 0.1435 - accuracy: 0.9544\n",
      "Epoch 9/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.1339 - accuracy: 0.9563WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 7s 4ms/sample - loss: 0.1337 - accuracy: 0.9563\n",
      "Epoch 10/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9585WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 7s 4ms/sample - loss: 0.1270 - accuracy: 0.9587\n",
      "Epoch 11/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9613WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 8s 4ms/sample - loss: 0.1198 - accuracy: 0.9613\n",
      "Epoch 12/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.1137 - accuracy: 0.9627WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 8s 4ms/sample - loss: 0.1135 - accuracy: 0.9628\n",
      "Epoch 13/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.1083 - accuracy: 0.9643WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 8s 4ms/sample - loss: 0.1087 - accuracy: 0.9642\n",
      "Epoch 14/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.1027 - accuracy: 0.9662WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 8s 5ms/sample - loss: 0.1026 - accuracy: 0.9662\n",
      "Epoch 15/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.0980 - accuracy: 0.9681WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 39s 21ms/sample - loss: 0.0982 - accuracy: 0.9681\n",
      "Epoch 16/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.0939 - accuracy: 0.9691WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 8s 5ms/sample - loss: 0.0937 - accuracy: 0.9691\n",
      "Epoch 17/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.0888 - accuracy: 0.9710WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 9s 5ms/sample - loss: 0.0887 - accuracy: 0.9711\n",
      "Epoch 18/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.0846 - accuracy: 0.9721WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 9s 5ms/sample - loss: 0.0845 - accuracy: 0.9721\n",
      "Epoch 19/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.0820 - accuracy: 0.9734WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 10s 5ms/sample - loss: 0.0819 - accuracy: 0.9734\n",
      "Epoch 20/20\n",
      "1792/1808 [============================>.] - ETA: 0s - loss: 0.0775 - accuracy: 0.9748WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1808/1808 [==============================] - 9s 5ms/sample - loss: 0.0777 - accuracy: 0.9747\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "val_size = 0\n",
    "history = train_model(model,x_train,y_train,batch_size=batch_size,epochs=epochs,val_split = val_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_mappings(ids):\n",
    "    return {str(i[1]):i[0] for i in ids.items()}\n",
    "\n",
    "def generate_sample(x,y,model):\n",
    "    idx = random.randint(0,len(x))\n",
    "    sample = x[idx]\n",
    "    label = np.argmax(y[idx],axis=1)\n",
    "\n",
    "    p = model.predict(sample.reshape(1,-1))\n",
    "    p = np.argmax(p,axis=-1)\n",
    "    print(\"{:25} {:20}: {:10}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "    print(\"-\"*50)\n",
    "    for i in range(len(sample)):\n",
    "        word = str(sample[i])\n",
    "        pred = str(p[0][i])\n",
    "        true_val = str(label[i])\n",
    "        id_to_words = get_id_mappings(word_ids)\n",
    "        id_to_tags = get_id_mappings(tag_ids)\n",
    "        print(f\"{id_to_words[word]:25}{id_to_tags[true_val]:20}{id_to_tags[pred]}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                      True                : Pred      \n",
      "--------------------------------------------------\n",
      "known                    O                   O\n",
      "lastname                 O                   O\n",
      "<3DigitNum>              O                   O\n",
      "reports                  O                   O\n",
      "sudden                   O                   O\n",
      "onset                    O                   O\n",
      "of                       O                   O\n",
      "substernal               BODY                BODY\n",
      "chest                    DOS                 DOS\n",
      "pain                     DOS                 DOS\n",
      "at                       O                   O\n",
      "<1DigitNum>              Time                Time\n",
      "am                       Time                Time\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n"
     ]
    }
   ],
   "source": [
    "generate_sample(x,y,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model gives reasonable predictions that almost always make sense intuitively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_ids_to_tags(preds,tag_ids):\n",
    "    id_to_tags = get_id_mappings(tag_ids)\n",
    "\n",
    "    tag_seqs = []\n",
    "    for seq in preds:\n",
    "        tag_seqs.append([id_to_tags[str(i)] for i in seq])\n",
    "    return tag_seqs\n",
    "\n",
    "def get_real_labels(model,x_test,y_test,tag_ids):\n",
    "    test_preds = np.argmax(model.predict(x_test),axis=-1)\n",
    "    true_vals = np.argmax(y_test,axis=-1)\n",
    "    test_preds = transform_ids_to_tags(test_preds,tag_ids)\n",
    "    true_vals = transform_ids_to_tags(true_vals,tag_ids)\n",
    "    return true_vals,test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Report \n",
      " ------------------------------------------------------------\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Time       0.43      0.36      0.39        28\n",
      "               Age       1.00      0.94      0.97        17\n",
      "               GEO       0.83      0.90      0.86        42\n",
      "               DOS       0.43      0.52      0.47       157\n",
      "          Duration       0.36      0.47      0.41        17\n",
      "              Drug       0.80      0.92      0.86        49\n",
      "         Condition       0.58      0.66      0.62        83\n",
      "  Test / Screening       0.69      0.65      0.67        37\n",
      "  Respiratory Rate       0.86      0.60      0.71        10\n",
      "Patient Relocation       0.84      0.96      0.90        27\n",
      "         Procedure       0.38      0.55      0.45        42\n",
      "          Quantity       0.33      0.33      0.33         9\n",
      "    Blood Pressure       0.75      0.94      0.83        32\n",
      "       Temperature       0.83      0.71      0.77         7\n",
      "              Date       0.58      0.82      0.68        17\n",
      "              Dose       0.65      0.60      0.63        25\n",
      "        Heart Rate       0.83      0.91      0.87        11\n",
      "            Gender       1.00      1.00      1.00        11\n",
      "  Accident / Event       0.33      0.33      0.33        12\n",
      "              BODY       0.73      0.61      0.67        31\n",
      "               POI       0.50      0.60      0.55         5\n",
      "             Route       1.00      0.75      0.86         4\n",
      "         Frequency       0.40      0.29      0.33         7\n",
      "\n",
      "         micro avg       0.60      0.66      0.63       680\n",
      "         macro avg       0.61      0.66      0.63       680\n",
      "\n",
      "Train Report \n",
      " ------------------------------------------------------------\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "    Blood Pressure       0.90      0.99      0.95       299\n",
      "               GEO       0.91      0.93      0.92       484\n",
      "       Temperature       0.93      0.89      0.91        70\n",
      "               POI       0.88      0.89      0.88       138\n",
      "              Dose       0.90      0.91      0.91       239\n",
      "               DOS       0.77      0.84      0.80      1431\n",
      "              Drug       0.92      0.94      0.93       672\n",
      "          Quantity       0.80      0.77      0.78       115\n",
      "         Condition       0.83      0.86      0.84       739\n",
      "  Test / Screening       0.91      0.91      0.91       347\n",
      "         Procedure       0.80      0.84      0.82       397\n",
      "              Time       0.80      0.79      0.79       221\n",
      "Patient Relocation       0.87      0.95      0.91       303\n",
      "  Accident / Event       0.70      0.75      0.73        69\n",
      "             Route       0.96      0.94      0.95       160\n",
      "              Date       0.91      0.96      0.94       188\n",
      "              BODY       0.88      0.91      0.89       487\n",
      "            Gender       0.99      1.00      1.00       106\n",
      "         Frequency       0.76      0.71      0.73        78\n",
      "        Heart Rate       0.82      0.79      0.81        68\n",
      "          Duration       0.70      0.78      0.74       143\n",
      "               Age       0.98      0.97      0.97       125\n",
      "  Respiratory Rate       0.91      0.80      0.85        49\n",
      "\n",
      "         micro avg       0.85      0.88      0.87      6928\n",
      "         macro avg       0.85      0.88      0.87      6928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_test_vals,test_preds = get_real_labels(model,x_test,y_test,tag_ids)\n",
    "true_train_vals,train_preds = get_real_labels(model,x_train,y_train,tag_ids)\n",
    "test_report = classification_report(true_test_vals,test_preds)\n",
    "train_report = classification_report(true_train_vals,train_preds)\n",
    "print(\"Test Report \\n\",\"-\"*60)\n",
    "print(test_report)\n",
    "print(\"Train Report \\n\",\"-\"*60)\n",
    "print(train_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model still struggles with rarer classes such as frequency, age and gender but does very well determining tags sycg as DRUG and LOCATION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.6282140375260596\n"
     ]
    }
   ],
   "source": [
    "model_f1 = f1_score(true_test_vals,test_preds)\n",
    "print(\"F1-Score:\",model_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Saving Model Results\n",
    "\n",
    "In order to track progression its good to document each model iteration as well as keep note of important changes in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(x)\n",
    "model_desc = f\"BiLSTM-Word2Vec-EmbedSize-{embedding_size}\"\n",
    "results_file = \"./data/model_results.csv\"\n",
    "note = '''20 Epochs no validation'''\n",
    "def append_model_results(model_f1,n_samples,model_desc,file,note):\n",
    "    with open(file,'a') as f:\n",
    "        results = f\"\\n{model_f1},{n_samples},{model_desc},{time.ctime()},{note}\"\n",
    "        f.writelines(results)\n",
    "    print(\"~~~Results Successfully Saved\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~Results Successfully Saved\n"
     ]
    }
   ],
   "source": [
    "append_model_results(model_f1,n_samples,model_desc,results_file,note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>n-sample</th>\n",
       "      <th>Model Type</th>\n",
       "      <th>Date</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.524544</td>\n",
       "      <td>1008</td>\n",
       "      <td>BiLSTM-Word2Vec-EmbedSize-100</td>\n",
       "      <td>Tue Nov 12 13:45:02 2019</td>\n",
       "      <td>Used Custom Word2Vec Embeddings of entire Disc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.587530</td>\n",
       "      <td>1008</td>\n",
       "      <td>BiLSTM-Word2Vec-EmbedSize-100</td>\n",
       "      <td>Tue Nov 12 13:45:40 2019</td>\n",
       "      <td>Custom Word2Vec of entire corpus with numeric ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.608212</td>\n",
       "      <td>2009</td>\n",
       "      <td>BiLSTM-Word2Vec-EmbedSize-100</td>\n",
       "      <td>Thu Nov 14 13:07:01 2019</td>\n",
       "      <td>Custom Word2Vec of entire corpus with numeric ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.618051</td>\n",
       "      <td>2009</td>\n",
       "      <td>BiLSTM-Word2Vec-EmbedSize-100</td>\n",
       "      <td>Thu Nov 14 13:14:45 2019</td>\n",
       "      <td>Same as last with bug fix for slicing token co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.628214</td>\n",
       "      <td>2009</td>\n",
       "      <td>BiLSTM-Word2Vec-EmbedSize-100</td>\n",
       "      <td>Thu Nov 14 13:25:56 2019</td>\n",
       "      <td>20 Epochs no validation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   F1-Score  n-sample                     Model Type  \\\n",
       "0  0.524544      1008  BiLSTM-Word2Vec-EmbedSize-100   \n",
       "1  0.587530      1008  BiLSTM-Word2Vec-EmbedSize-100   \n",
       "2  0.608212      2009  BiLSTM-Word2Vec-EmbedSize-100   \n",
       "3  0.618051      2009  BiLSTM-Word2Vec-EmbedSize-100   \n",
       "4  0.628214      2009  BiLSTM-Word2Vec-EmbedSize-100   \n",
       "\n",
       "                       Date                                              Notes  \n",
       "0  Tue Nov 12 13:45:02 2019  Used Custom Word2Vec Embeddings of entire Disc...  \n",
       "1  Tue Nov 12 13:45:40 2019  Custom Word2Vec of entire corpus with numeric ...  \n",
       "2  Thu Nov 14 13:07:01 2019  Custom Word2Vec of entire corpus with numeric ...  \n",
       "3  Thu Nov 14 13:14:45 2019  Same as last with bug fix for slicing token co...  \n",
       "4  Thu Nov 14 13:25:56 2019                            20 Epochs no validation  "
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.read_csv(results_file)\n",
    "results_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras36]",
   "language": "python",
   "name": "conda-env-keras36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
