{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation Pipeline for DataTurks .tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from seqeval.metrics import f1_score,classification_report\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional,Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Data to Sequences\n",
    "\n",
    "### Contents:\n",
    "\n",
    "1. Reading File and formatting as sequences\n",
    "2. Formatting Entities to IOB Scheme\n",
    "3. Padding Sequences\n",
    "4. Mapping to Integer Ids\n",
    "5. Formatting Data for Keras LSTM Model\n",
    "6. Train Test Split\n",
    "7. Specifying Model and Model Parameters\n",
    "8. Training Model\n",
    "9. Evaluating Model\n",
    "10. Saving Model Results\n",
    "\n",
    "### 1. Reading and Formatting File:\n",
    "\n",
    "The file being used is the raw output of a data turks annotated tsv file.\n",
    "\n",
    "More info Available:\n",
    "https://dataturks.com/features/document-ner-annotation.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_turks(file):\n",
    "    with open(file) as f:\n",
    "        lines = [i.rstrip().split(\"\\t\") for i in f.readlines()]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0,\"HISTORY', 'O'],\n",
       " ['OF', 'O'],\n",
       " ['PRESENT', 'O'],\n",
       " ['ILLNESS', 'O'],\n",
       " ['This', 'O'],\n",
       " ['is', 'O'],\n",
       " ['an', 'O'],\n",
       " ['81-year-old', 'CONDITION/SYMPTOM'],\n",
       " ['female', 'CONDITION/SYMPTOM'],\n",
       " ['with', 'O'],\n",
       " ['a', 'O'],\n",
       " ['history', 'O'],\n",
       " ['of', 'O'],\n",
       " ['emphysema', 'CONDITION/SYMPTOM'],\n",
       " ['not', 'O'],\n",
       " ['on', 'O'],\n",
       " ['home', 'DRUG'],\n",
       " ['O2', 'DRUG'],\n",
       " [',', 'O'],\n",
       " ['who', 'O']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"./nlp_data/Medical NER Dataset 2600.tsv\"\n",
    "word_ents = read_turks(file)\n",
    "word_ents[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words (such as the first above) contained a number and quote before it so these are removed with the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(word_ents):\n",
    "    '''removes quote and comma characters from'''\n",
    "    new_word_ents = []\n",
    "    for ents in word_ents:\n",
    "        word = ents[0].lower()\n",
    "        if word.find(',') > 0:\n",
    "            word = word[word.find(',')+1:]\n",
    "        word = word.replace('\"','')\n",
    "        ents[0] = word\n",
    "        new_word_ents.append(ents)\n",
    "    return new_word_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['history', 'O'],\n",
       " ['of', 'O'],\n",
       " ['present', 'O'],\n",
       " ['illness', 'O'],\n",
       " ['this', 'O'],\n",
       " ['is', 'O'],\n",
       " ['an', 'O'],\n",
       " ['81-year-old', 'CONDITION/SYMPTOM'],\n",
       " ['female', 'CONDITION/SYMPTOM'],\n",
       " ['with', 'O'],\n",
       " ['a', 'O'],\n",
       " ['history', 'O'],\n",
       " ['of', 'O'],\n",
       " ['emphysema', 'CONDITION/SYMPTOM'],\n",
       " ['not', 'O'],\n",
       " ['on', 'O'],\n",
       " ['home', 'DRUG'],\n",
       " ['o2', 'DRUG'],\n",
       " [',', 'O'],\n",
       " ['who', 'O']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ents = clean_words(word_ents)\n",
    "new_ents[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataturks uses a blank line to seperate each sequence. This is why most csv/tsv readers cannot read the file. The following function will split each sequence when it finds a blank line in the tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seqs(word_ents):\n",
    "    seqs = []\n",
    "    seq = []\n",
    "    for ents in word_ents:\n",
    "        if len(ents)>1:\n",
    "            if len(ents[0])>0:\n",
    "                seq.append(ents)\n",
    "        else:\n",
    "            seqs.append(seq)\n",
    "            seq=[]\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['history', 'O'],\n",
       "  ['of', 'O'],\n",
       "  ['present', 'O'],\n",
       "  ['illness', 'O'],\n",
       "  ['this', 'O'],\n",
       "  ['is', 'O'],\n",
       "  ['an', 'O'],\n",
       "  ['81-year-old', 'CONDITION/SYMPTOM'],\n",
       "  ['female', 'CONDITION/SYMPTOM'],\n",
       "  ['with', 'O'],\n",
       "  ['a', 'O'],\n",
       "  ['history', 'O'],\n",
       "  ['of', 'O'],\n",
       "  ['emphysema', 'CONDITION/SYMPTOM'],\n",
       "  ['not', 'O'],\n",
       "  ['on', 'O'],\n",
       "  ['home', 'DRUG'],\n",
       "  ['o2', 'DRUG'],\n",
       "  [',', 'O'],\n",
       "  ['who', 'O'],\n",
       "  ['presents', 'O'],\n",
       "  ['with', 'O'],\n",
       "  ['three', 'AMOUNT'],\n",
       "  ['days', 'AMOUNT'],\n",
       "  ['of', 'O'],\n",
       "  ['shortness', 'CONDITION/SYMPTOM'],\n",
       "  ['of', 'CONDITION/SYMPTOM'],\n",
       "  ['breath', 'CONDITION/SYMPTOM'],\n",
       "  ['thought', 'O'],\n",
       "  ['by', 'O'],\n",
       "  ['her', 'O'],\n",
       "  ['primary', 'O'],\n",
       "  ['care', 'O'],\n",
       "  ['doctor', 'O'],\n",
       "  ['to', 'O'],\n",
       "  ['be', 'O'],\n",
       "  ['a', 'O'],\n",
       "  ['copd', 'CONDITION/SYMPTOM'],\n",
       "  ['flare', 'CONDITION/SYMPTOM']]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = create_seqs(new_ents)\n",
    "seqs[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Formatting Entities to IOB (Inside,Outside, Beginning) Scheme \n",
    "\n",
    "This scheme adds more context to the tags and allows annotations to make more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tags(word_ents):\n",
    "    '''adds IOB scheme to tags'''\n",
    "    new_ents = []\n",
    "    for i in range(0,len(word_ents)):\n",
    "        if word_ents[i][1] == \"O\":\n",
    "            tag = word_ents[i][1]\n",
    "        else:\n",
    "            if not i:\n",
    "                tag = \"B-\"+word_ents[i][1]\n",
    "            else:\n",
    "                if (word_ents[i][1] != word_ents[i-1][1]):\n",
    "                    tag = \"B-\"+word_ents[i][1]\n",
    "                else:\n",
    "                    tag = \"I-\"+word_ents[i][1]\n",
    "\n",
    "        new_ents.append([word_ents[i][0],tag])\n",
    "    return new_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['history', 'O'],\n",
       " ['of', 'O'],\n",
       " ['present', 'O'],\n",
       " ['illness', 'O'],\n",
       " ['this', 'O'],\n",
       " ['is', 'O'],\n",
       " ['an', 'O'],\n",
       " ['81-year-old', 'B-CONDITION/SYMPTOM'],\n",
       " ['female', 'I-CONDITION/SYMPTOM'],\n",
       " ['with', 'O'],\n",
       " ['a', 'O'],\n",
       " ['history', 'O'],\n",
       " ['of', 'O'],\n",
       " ['emphysema', 'B-CONDITION/SYMPTOM'],\n",
       " ['not', 'O'],\n",
       " ['on', 'O'],\n",
       " ['home', 'B-DRUG'],\n",
       " ['o2', 'I-DRUG'],\n",
       " [',', 'O'],\n",
       " ['who', 'O'],\n",
       " ['presents', 'O'],\n",
       " ['with', 'O'],\n",
       " ['three', 'B-AMOUNT'],\n",
       " ['days', 'I-AMOUNT'],\n",
       " ['of', 'O'],\n",
       " ['shortness', 'B-CONDITION/SYMPTOM'],\n",
       " ['of', 'I-CONDITION/SYMPTOM'],\n",
       " ['breath', 'I-CONDITION/SYMPTOM'],\n",
       " ['thought', 'O'],\n",
       " ['by', 'O'],\n",
       " ['her', 'O'],\n",
       " ['primary', 'O'],\n",
       " ['care', 'O'],\n",
       " ['doctor', 'O'],\n",
       " ['to', 'O'],\n",
       " ['be', 'O'],\n",
       " ['a', 'O'],\n",
       " ['copd', 'B-CONDITION/SYMPTOM'],\n",
       " ['flare', 'I-CONDITION/SYMPTOM']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tag_seqs = [clean_tags(ents) for ents in seqs]\n",
    "cleaned_tag_seqs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Padding Sequences to a Specified Length\n",
    "\n",
    "In order to be usable by the LSTM model, each sequence needs to be padded/truncated to the same length. Here 50 is chosen somewhat arbitraily but is around the 97th percentile of sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(seq,max_len):\n",
    "    padded_seq = seq+[[\"<PAD>\",\"O\"]]*max_len\n",
    "    return padded_seq[:max_len]\n",
    "    \n",
    "def pad_sequences(sequences,max_len=None):\n",
    "    if max_len == None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "    return [pad_seq(seq,max_len) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['history', 'O'],\n",
       " ['of', 'O'],\n",
       " ['present', 'O'],\n",
       " ['illness', 'O'],\n",
       " ['this', 'O'],\n",
       " ['is', 'O'],\n",
       " ['an', 'O'],\n",
       " ['81-year-old', 'B-CONDITION/SYMPTOM'],\n",
       " ['female', 'I-CONDITION/SYMPTOM'],\n",
       " ['with', 'O'],\n",
       " ['a', 'O'],\n",
       " ['history', 'O'],\n",
       " ['of', 'O'],\n",
       " ['emphysema', 'B-CONDITION/SYMPTOM'],\n",
       " ['not', 'O'],\n",
       " ['on', 'O'],\n",
       " ['home', 'B-DRUG'],\n",
       " ['o2', 'I-DRUG'],\n",
       " [',', 'O'],\n",
       " ['who', 'O'],\n",
       " ['presents', 'O'],\n",
       " ['with', 'O'],\n",
       " ['three', 'B-AMOUNT'],\n",
       " ['days', 'I-AMOUNT'],\n",
       " ['of', 'O'],\n",
       " ['shortness', 'B-CONDITION/SYMPTOM'],\n",
       " ['of', 'I-CONDITION/SYMPTOM'],\n",
       " ['breath', 'I-CONDITION/SYMPTOM'],\n",
       " ['thought', 'O'],\n",
       " ['by', 'O'],\n",
       " ['her', 'O'],\n",
       " ['primary', 'O'],\n",
       " ['care', 'O'],\n",
       " ['doctor', 'O'],\n",
       " ['to', 'O'],\n",
       " ['be', 'O'],\n",
       " ['a', 'O'],\n",
       " ['copd', 'B-CONDITION/SYMPTOM'],\n",
       " ['flare', 'I-CONDITION/SYMPTOM'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 50\n",
    "padded_seqs = pad_sequences(cleaned_tag_seqs,max_len)\n",
    "padded_seqs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Mapping Words to Integer Values for Model Training\n",
    "\n",
    "The model can't use words so each one is mapped to a particular index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ids(sentances,tag=False):\n",
    "    words = []\n",
    "    for sentance in sentances:\n",
    "        words += list([word[tag] for word in sentance])\n",
    "    word_dict = {word:i for i,word in enumerate(set(words))}\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('developing', 0), ('postprandially', 1), ('reporting', 2), ('headache', 3)]\n",
      "[('I-CONDITION/SYMPTOM', 0), ('B-CONDITION/SYMPTOM', 1), ('B-LOCATION', 2), ('I-DRUG', 3)]\n"
     ]
    }
   ],
   "source": [
    "word_ids = get_word_ids(padded_seqs)\n",
    "tag_ids = get_word_ids(padded_seqs,tag=True)\n",
    "print(list(word_ids.items())[:4])\n",
    "print(list(tag_ids.items())[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_ids(sentances,word_ids,tag_ids):\n",
    "    vector = []\n",
    "    for sentance in sentances:\n",
    "        vector.append(list([[word_ids[w[0]],tag_ids[w[1]]] for w in sentance]))\n",
    "    return np.array(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the words are given a numeric representation which can be mapped back to the original words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2354   17]\n",
      " [2216   17]\n",
      " [5080   17]\n",
      " [3238   17]]\n",
      "\n",
      "Word Representation:\n",
      "[['history', 'O'], ['of', 'O'], ['present', 'O'], ['illness', 'O']]\n"
     ]
    }
   ],
   "source": [
    "vectors = words_to_ids(padded_seqs,word_ids,tag_ids)\n",
    "print(vectors[0][:4])\n",
    "print('')\n",
    "print(\"Word Representation:\")\n",
    "print(padded_seqs[0][:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can label our features (x) and labels (y) for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x_y(matrix,n_tags):\n",
    "    x = []\n",
    "    y = []\n",
    "    for sequences in matrix:\n",
    "        xi = [i[0] for i in sequences]\n",
    "        yi = [i[1] for i in sequences]\n",
    "        x.append(xi)\n",
    "        y.append(yi)\n",
    "    y = np.array([to_categorical(i,n_tags) for i in y])\n",
    "    return np.array(x),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-shape: (2592, 50)\n",
      "[2354 2216 5080 3238  964]\n",
      "\n",
      "Y-shape: (2592, 50, 23)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "n_tags = len(tag_ids)\n",
    "x,y = create_x_y(vectors,n_tags)\n",
    "print(\"X-shape:\",x.shape)\n",
    "print(x[0][:5])\n",
    "print('')\n",
    "print(\"Y-shape:\",y.shape)\n",
    "print(y[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Specifying Model and Model Parameters\n",
    "\n",
    "https://www.depends-on-the-definition.com/introduction-named-entity-recognition-python/\n",
    "\n",
    "#### Use Word2Vec Embedding Trained on Entire Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/LiamRoberts/anaconda3/envs/keras36/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "/Users/LiamRoberts/anaconda3/envs/keras36/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "def create_weight_matrix(word_ids,embeddings_index):\n",
    "    embedding_matrix = np.zeros((len(word_ids),100))\n",
    "    count = 0\n",
    "    oov_words = []\n",
    "    for word,idx in word_ids.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[idx] = embeddings_index[word]\n",
    "        else:\n",
    "            oov_words.append(word)\n",
    "    return embedding_matrix,oov_words    \n",
    "\n",
    "custom_emb = Word2Vec.load(\"./nlp_data/word2vec.model\")\n",
    "embed_matrix,oov_words = create_weight_matrix(word_ids,custom_emb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent OOV: 0.9389671361502347%\n",
      "['ctaxol', '181/64', ',600', 'bp81/20', '186/124', '?pancreatitis', 'hyperlipidema', '3996', 'bp111/46', '139/88', 't95', '7.37/58/96', \"100-170's\", '118/54', 'diaphoeris', 'throughou', 'cp/sob/doe/f/c/n/v/brbpr/melena', '400/20/5/0.6', 'tasty', '30s-50s', 'hemeturia', '/wnl', '186/84', 'hypercacemia', 'sbp160-180s', '167/60', '78-129', '5805', 'obligate', 'infetion']\n"
     ]
    }
   ],
   "source": [
    "# OOV Words\n",
    "print(f\"Percent OOV: {len(oov_words)/len(word_ids)*100}%\")\n",
    "print(oov_words[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of how some words are split there are still some words out of vocabulary even though the Word2Vec embeddings were trained on the full  version of the same corpus. Each of these OOV words will simply have 0 weights in the matrix. The 0.9% of OOV words is a huge improvement over what was seen using Glove embeddings which saw 13% of the vocab being OOV words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_BiLSTM(n_words,n_tags,embedding_size,max_len,embed_weights):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(n_words,\n",
    "                        embedding_size,\n",
    "                        weights=[embed_weights],\n",
    "                        trainable=False,\n",
    "                        input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.2)))\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"softmax\")))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 100)           532500    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 50, 200)           160800    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 50, 23)            4623      \n",
      "=================================================================\n",
      "Total params: 697,923\n",
      "Trainable params: 165,423\n",
      "Non-trainable params: 532,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 100\n",
    "n_words = len(word_ids)\n",
    "n_tags = len(tag_ids)\n",
    "\n",
    "model = create_BiLSTM(n_words,n_tags,embedding_size,max_len,embed_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model(model,x_train,y_train,batch_size=32,epochs=20,val_split = 0.1):\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0.0001,\n",
    "                               patience=3,\n",
    "                               mode='min',\n",
    "                               verbose=1)\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit(x_train, y_train, \n",
    "                        batch_size=32, \n",
    "                        epochs=epochs, \n",
    "                        validation_split=val_split, \n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stop]\n",
    "                       )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2098 samples, validate on 234 samples\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From /Users/LiamRoberts/anaconda3/envs/keras36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1a3f3e31e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1a3f3e31e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "2098/2098 [==============================] - 12s 6ms/sample - loss: 0.5959 - accuracy: 0.8666 - val_loss: 0.3226 - val_accuracy: 0.9062\n",
      "Epoch 2/50\n",
      "2098/2098 [==============================] - 10s 5ms/sample - loss: 0.2873 - accuracy: 0.9135 - val_loss: 0.2658 - val_accuracy: 0.9198\n",
      "Epoch 3/50\n",
      "2098/2098 [==============================] - 10s 5ms/sample - loss: 0.2398 - accuracy: 0.9263 - val_loss: 0.2303 - val_accuracy: 0.9286\n",
      "Epoch 4/50\n",
      "2098/2098 [==============================] - 10s 5ms/sample - loss: 0.2134 - accuracy: 0.9335 - val_loss: 0.2210 - val_accuracy: 0.9342\n",
      "Epoch 5/50\n",
      "2098/2098 [==============================] - 10s 5ms/sample - loss: 0.1969 - accuracy: 0.9384 - val_loss: 0.2120 - val_accuracy: 0.9360\n",
      "Epoch 6/50\n",
      "2098/2098 [==============================] - 9s 4ms/sample - loss: 0.1857 - accuracy: 0.9416 - val_loss: 0.2030 - val_accuracy: 0.9383\n",
      "Epoch 7/50\n",
      "2098/2098 [==============================] - 10s 5ms/sample - loss: 0.1737 - accuracy: 0.9450 - val_loss: 0.1993 - val_accuracy: 0.9384\n",
      "Epoch 8/50\n",
      "2098/2098 [==============================] - 10s 5ms/sample - loss: 0.1648 - accuracy: 0.9470 - val_loss: 0.1994 - val_accuracy: 0.9385\n",
      "Epoch 9/50\n",
      "2098/2098 [==============================] - 10s 5ms/sample - loss: 0.1573 - accuracy: 0.9491 - val_loss: 0.1936 - val_accuracy: 0.9412\n",
      "Epoch 10/50\n",
      "2098/2098 [==============================] - 9s 4ms/sample - loss: 0.1504 - accuracy: 0.9514 - val_loss: 0.1917 - val_accuracy: 0.9419\n",
      "Epoch 11/50\n",
      "2098/2098 [==============================] - 10s 5ms/sample - loss: 0.1445 - accuracy: 0.9529 - val_loss: 0.1905 - val_accuracy: 0.9433\n",
      "Epoch 12/50\n",
      "2098/2098 [==============================] - 9s 4ms/sample - loss: 0.1386 - accuracy: 0.9545 - val_loss: 0.1928 - val_accuracy: 0.9421\n",
      "Epoch 13/50\n",
      "2098/2098 [==============================] - 9s 4ms/sample - loss: 0.1343 - accuracy: 0.9561 - val_loss: 0.1904 - val_accuracy: 0.9423\n",
      "Epoch 14/50\n",
      "2098/2098 [==============================] - 9s 4ms/sample - loss: 0.1297 - accuracy: 0.9576 - val_loss: 0.1921 - val_accuracy: 0.9433\n",
      "Epoch 15/50\n",
      "2098/2098 [==============================] - 9s 4ms/sample - loss: 0.1251 - accuracy: 0.9592 - val_loss: 0.1906 - val_accuracy: 0.9418\n",
      "Epoch 16/50\n",
      "2098/2098 [==============================] - 10s 5ms/sample - loss: 0.1188 - accuracy: 0.9609 - val_loss: 0.1927 - val_accuracy: 0.9422\n",
      "Epoch 00016: early stopping\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 32\n",
    "history = train_model(model,x_train,y_train,batch_size=batch_size,epochs=epochs,val_split = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_mappings(ids):\n",
    "    return {str(i[1]):i[0] for i in ids.items()}\n",
    "\n",
    "def generate_sample(x,y,model):\n",
    "    idx = random.randint(0,len(x))\n",
    "    sample = x[idx]\n",
    "    label = np.argmax(y[idx],axis=1)\n",
    "\n",
    "    p = model.predict(sample.reshape(1,-1))\n",
    "    p = np.argmax(p,axis=-1)\n",
    "    print(\"{:25} {:20}: {:10}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "    print(\"-\"*50)\n",
    "    for i in range(len(sample)):\n",
    "        word = str(sample[i])\n",
    "        pred = str(p[0][i])\n",
    "        true_val = str(label[i])\n",
    "        id_to_words = get_id_mappings(word_ids)\n",
    "        id_to_tags = get_id_mappings(tag_ids)\n",
    "        print(f\"{id_to_words[word]:25}{id_to_tags[true_val]:20}{id_to_tags[pred]}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                      True                : Pred      \n",
      "--------------------------------------------------\n",
      "he                       O                   O\n",
      "states                   O                   O\n",
      "that                     O                   O\n",
      "he                       O                   O\n",
      "is                       O                   O\n",
      "asymptomatic             B-CONDITION/SYMPTOM B-CONDITION/SYMPTOM\n",
      "with                     I-CONDITION/SYMPTOM I-CONDITION/SYMPTOM\n",
      "these                    I-CONDITION/SYMPTOM I-CONDITION/SYMPTOM\n",
      "sugars                   I-CONDITION/SYMPTOM I-CONDITION/SYMPTOM\n",
      ",                        O                   O\n",
      "but                      O                   O\n",
      "his                      O                   O\n",
      "wife                     O                   O\n",
      "says                     O                   O\n",
      "he's                     O                   O\n",
      "been                     O                   O\n",
      "sleepier                 B-CONDITION/SYMPTOM O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n",
      "<PAD>                    O                   O\n"
     ]
    }
   ],
   "source": [
    "generate_sample(x,y,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model gives reasonable predictions that almost always make sense intuitively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_ids_to_tags(preds,tag_ids):\n",
    "    id_to_tags = get_id_mappings(tag_ids)\n",
    "\n",
    "    tag_seqs = []\n",
    "    for seq in preds:\n",
    "        tag_seqs.append([id_to_tags[str(i)] for i in seq])\n",
    "    return tag_seqs\n",
    "\n",
    "def get_real_labels(model,x_test,y_test,tag_ids):\n",
    "    test_preds = np.argmax(model.predict(x_test),axis=-1)\n",
    "    true_vals = np.argmax(y_test,axis=-1)\n",
    "    test_preds = transform_ids_to_tags(test_preds,tag_ids)\n",
    "    true_vals = transform_ids_to_tags(true_vals,tag_ids)\n",
    "    return true_vals,test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "CONDITION/SYMPTOM       0.57      0.64      0.61       348\n",
      "         LOCATION       0.72      0.81      0.76       101\n",
      "      MEASUREMENT       0.60      0.69      0.64        96\n",
      "             TIME       0.49      0.54      0.51        37\n",
      "           AMOUNT       0.68      0.62      0.65        68\n",
      "            EVENT       0.45      0.55      0.49       119\n",
      "             DRUG       0.79      0.80      0.80       100\n",
      "        FREQUENCY       0.40      0.17      0.24        12\n",
      "           GENDER       0.50      0.36      0.42        11\n",
      "              AGE       0.46      0.40      0.43        15\n",
      "     ORGANIZATION       0.57      0.62      0.59        13\n",
      "\n",
      "        micro avg       0.60      0.65      0.62       920\n",
      "        macro avg       0.60      0.65      0.62       920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_vals,test_preds = get_real_labels(model,x_test,y_test,tag_ids)\n",
    "report = classification_report(true_vals,test_preds)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model still struggles with rarer classes such as frequency, age and gender but does very well determining tags sycg as DRUG and LOCATION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.6229166666666667\n"
     ]
    }
   ],
   "source": [
    "model_f1 = f1_score(true_vals,test_preds)\n",
    "print(\"F1-Score:\",model_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Saving Model Results\n",
    "\n",
    "In order to track progression its good to document each model iteration as well as keep note of important changes in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(x)\n",
    "model_desc = f\"BiLSTM-Word2Vec-EmbedSize-{embedding_size}\"\n",
    "results_file = \"./nlp_data/model_results.csv\"\n",
    "note = '''Used Custom Word2Vec Embeddings of entire Discharge Summary Corpus'''\n",
    "def append_model_results(model_f1,n_samples,model_desc,file,note):\n",
    "    with open(file,'a') as f:\n",
    "        results = f\"\\n{model_f1},{n_samples},{model_desc},{time.ctime()},{note}\"\n",
    "        f.writelines(results)\n",
    "    print(\"~~~Results Successfully Saved\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~Results Successfully Saved\n"
     ]
    }
   ],
   "source": [
    "append_model_results(model_f1,n_samples,model_desc,results_file,note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>N-Samples</th>\n",
       "      <th>Model Type</th>\n",
       "      <th>Date</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.567810</td>\n",
       "      <td>2592.0</td>\n",
       "      <td>BiLSTM-Glove-EmbedSize-100</td>\n",
       "      <td>Thu Nov  7 11:13:40 2019</td>\n",
       "      <td>note</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.567810</td>\n",
       "      <td>2592.0</td>\n",
       "      <td>BiLSTM-Glove-EmbedSize-100</td>\n",
       "      <td>Thu Nov  7 11:13:48 2019</td>\n",
       "      <td>Split Special Characters to reduce OOV Glove E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.606472</td>\n",
       "      <td>2592.0</td>\n",
       "      <td>BiLSTM-Word2Vec-EmbedSize-100</td>\n",
       "      <td>Thu Nov  7 13:30:13 2019</td>\n",
       "      <td>Reduced Max Sequence Length to 43 (95th percen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.606472</td>\n",
       "      <td>2592.0</td>\n",
       "      <td>BiLSTM-Word2Vec-EmbedSize-100</td>\n",
       "      <td>Thu Nov  7 13:30:51 2019</td>\n",
       "      <td>Used Custom Word2Vec Embeddings of entire Disc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.622917</td>\n",
       "      <td>2592.0</td>\n",
       "      <td>BiLSTM-Word2Vec-EmbedSize-100</td>\n",
       "      <td>Thu Nov  7 13:46:03 2019</td>\n",
       "      <td>Used Custom Word2Vec Embeddings of entire Disc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    F1-Score  N-Samples                     Model Type  \\\n",
       "11  0.567810     2592.0     BiLSTM-Glove-EmbedSize-100   \n",
       "12  0.567810     2592.0     BiLSTM-Glove-EmbedSize-100   \n",
       "13  0.606472     2592.0  BiLSTM-Word2Vec-EmbedSize-100   \n",
       "14  0.606472     2592.0  BiLSTM-Word2Vec-EmbedSize-100   \n",
       "15  0.622917     2592.0  BiLSTM-Word2Vec-EmbedSize-100   \n",
       "\n",
       "                        Date  \\\n",
       "11  Thu Nov  7 11:13:40 2019   \n",
       "12  Thu Nov  7 11:13:48 2019   \n",
       "13  Thu Nov  7 13:30:13 2019   \n",
       "14  Thu Nov  7 13:30:51 2019   \n",
       "15  Thu Nov  7 13:46:03 2019   \n",
       "\n",
       "                                                 Note  \n",
       "11                                               note  \n",
       "12  Split Special Characters to reduce OOV Glove E...  \n",
       "13  Reduced Max Sequence Length to 43 (95th percen...  \n",
       "14  Used Custom Word2Vec Embeddings of entire Disc...  \n",
       "15  Used Custom Word2Vec Embeddings of entire Disc...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.read_csv(results_file)\n",
    "results_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras36]",
   "language": "python",
   "name": "conda-env-keras36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
