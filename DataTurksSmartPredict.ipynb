{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Annotate Training Data with CRF and Labelled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from seqeval.metrics import f1_score,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import pos_tag\n",
    "from sklearn_crfsuite import CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pipeline for using previously annotated dataturks .tsv data to preannotate unlabelled sentances which can then be uploaded to dataturks and fixed accordingly. The purpose is to speed up the annotation process by annotating tags which the model can consistently predict as a human would and using human annotations to occasionally fix these tags and label more difficult tags.\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "#### 1. Format Annotated Turks TSV File for CRF Model\n",
    "#### 2. Train / Eval CRF Model\n",
    "#### 3. Format Unannotated Sentances for Prediction\n",
    "#### 4. Create Annotations\n",
    "#### 5. Format Annotations for Dataturks Ingestion\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Format Annotated Turks TSV File for CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_turks(file):\n",
    "    with open(file) as f:\n",
    "        lines = [i.rstrip().split(\"\\t\") for i in f.readlines()]\n",
    "    return lines\n",
    "\n",
    "def create_seqs(word_ents):\n",
    "    seqs = []\n",
    "    seq = []\n",
    "    for ents in word_ents:\n",
    "        if len(ents)>1:\n",
    "            if len(ents[0])>0:\n",
    "                if ents[0][-1] == \".\":\n",
    "                    seq.append([ents[0][:-1],ents[1]])\n",
    "                if len(ents[0])>1:\n",
    "                    seq.append([ents[0].replace(\",\",\"\"),ents[1]])\n",
    "                else:\n",
    "                    seq.append(ents)\n",
    "        else:\n",
    "            seqs.append([i for i in seq if len(i[0])>0])\n",
    "            seq=[]\n",
    "    return seqs\n",
    "\n",
    "def clean_tags(word_ents):\n",
    "    '''adds IOB scheme to tags'''\n",
    "    new_ents = []\n",
    "    for i in range(0,len(word_ents)):\n",
    "        if word_ents[i][1] == \"O\":\n",
    "            tag = word_ents[i][1]\n",
    "        else:\n",
    "            if not i:\n",
    "                tag = \"B-\"+word_ents[i][1]\n",
    "            else:\n",
    "                if (word_ents[i][1] != word_ents[i-1][1]):\n",
    "                    tag = \"B-\"+word_ents[i][1]\n",
    "                else:\n",
    "                    tag = \"I-\"+word_ents[i][1]\n",
    "\n",
    "        new_ents.append([word_ents[i][0],tag])\n",
    "    return new_ents\n",
    "\n",
    "def add_pos(seqs):\n",
    "    new_seqs = []\n",
    "    for sentance in seqs:\n",
    "        words = [word[0] for word in sentance]\n",
    "        pos = pos_tag(words)        \n",
    "        new_seq = [pos[i]+(sentance[i][1],) for i in range(len(sentance))]\n",
    "        new_seqs.append(new_seq)\n",
    "    return new_seqs\n",
    "\n",
    "def word2features(sent, i):\n",
    "    '''\n",
    "    \n",
    "    From:\n",
    "    https://www.depends-on-the-definition.com/named-entity-recognition-conditional-random-fields-python/\n",
    "    \n",
    "    '''\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n",
    "\n",
    "def create_crf_data(file):\n",
    "    seqs = read_turks(file)\n",
    "    seqs = create_seqs(seqs)\n",
    "    seqs = [clean_tags(ents) for ents in seqs]\n",
    "    seqs = add_pos(seqs)\n",
    "    x = [sent2features(s) for s in seqs]\n",
    "    y = [sent2labels(s) for s in seqs]\n",
    "    tokens = [sent2tokens(s) for s in seqs]\n",
    "    return x,y,tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-Sample (single value in sequence):\n",
      "{'bias': 1.0, 'word.lower()': '\"history', 'word[-3:]': 'ORY', 'word[-2:]': 'RY', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'NN', 'postag[:2]': 'NN', 'BOS': True, '+1:word.lower()': 'of', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'NNP', '+1:postag[:2]': 'NN'}\n",
      "\n",
      "Full Sequence Tokens:\n",
      "['\"HISTORY', 'OF', 'PRESENT', 'ILLNESS', 'This', 'is', 'an', '81-year-old', 'female', 'with', 'a', 'history', 'of', 'emphysema', 'not', 'on', 'home', 'O2', ',', 'who', 'presents', 'with', 'three', 'days', 'of', 'shortness', 'of', 'breath', 'thought', 'by', 'her', 'primary', 'care', 'doctor', 'to', 'be', 'a', 'COPD', 'flare', '\"']\n",
      "\n",
      "Full Sequence Labels:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Age', 'B-Gender', 'O', 'O', 'O', 'O', 'B-Condition', 'O', 'O', 'B-Drug', 'I-Drug', 'O', 'O', 'O', 'O', 'B-Duration', 'I-Duration', 'O', 'B-DOS', 'I-DOS', 'I-DOS', 'O', 'O', 'O', 'B-POI', 'I-POI', 'I-POI', 'O', 'O', 'O', 'B-Condition', 'I-Condition', 'O']\n"
     ]
    }
   ],
   "source": [
    "annotated_file = \"./data/Medical NER V2 4000.tsv\"\n",
    "x,y,tokens = create_crf_data(annotated_file)\n",
    "print(\"X-Sample (single value in sequence):\")\n",
    "print(x[0][0])\n",
    "print(\"\\nFull Sequence Tokens:\")\n",
    "print(tokens[0])\n",
    "print(\"\\nFull Sequence Labels:\")\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train / Eval CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_crf(x,y):\n",
    "    crf = CRF(algorithm='lbfgs',\n",
    "          c1=0.1,\n",
    "          c2=0.1,\n",
    "          max_iterations=100,\n",
    "          all_possible_transitions=False)\n",
    "\n",
    "    crf.fit(x, y)\n",
    "    \n",
    "    return crf\n",
    "\n",
    "def eval_crf(x,y,split_size = 0.1):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=split_size, random_state=42)\n",
    "    crf = train_crf(x_train,y_train)\n",
    "\n",
    "    # Evaluate Test Performance\n",
    "    pred = crf.predict(x_test)\n",
    "    report = classification_report(y_test,pred)\n",
    "    print(\"Test Results:\\n\",\"-\"*60)\n",
    "    print(report)\n",
    "    print(\"-\"*60)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      " ------------------------------------------------------------\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "                       DOS       0.69      0.66      0.67       357\n",
      "                 Procedure       0.66      0.57      0.61        91\n",
      "        Patient Relocation       0.81      0.81      0.81        83\n",
      "                      Date       0.85      0.81      0.83        42\n",
      "                      BODY       0.47      0.46      0.47        56\n",
      "Other Measurement / Result       0.73      0.63      0.68        81\n",
      "                      Drug       0.91      0.86      0.88       146\n",
      "                     Route       0.90      0.86      0.88        51\n",
      "                       GEO       0.87      0.86      0.86       118\n",
      "                      Time       0.66      0.70      0.68        67\n",
      "                 Condition       0.66      0.65      0.65       138\n",
      "               Temperature       0.82      0.69      0.75        13\n",
      "                  Duration       0.37      0.30      0.33        23\n",
      "                       POI       0.75      0.58      0.65        31\n",
      "          Respiratory Rate       1.00      0.80      0.89        10\n",
      "                 Frequency       0.78      0.44      0.56        16\n",
      "                       Age       1.00      1.00      1.00        20\n",
      "          Test / Screening       0.84      0.76      0.80        71\n",
      "             O2 Saturation       1.00      0.77      0.87        30\n",
      "          Accident / Event       0.64      0.50      0.56        14\n",
      "                Heart Rate       0.71      0.62      0.67        16\n",
      "                      Dose       0.97      0.94      0.95        65\n",
      "            Blood Pressure       0.78      0.82      0.80        22\n",
      "                  Quantity       0.73      0.50      0.59        22\n",
      "                    Gender       1.00      0.93      0.97        15\n",
      "\n",
      "                 micro avg       0.76      0.71      0.74      1598\n",
      "                 macro avg       0.76      0.71      0.74      1598\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "eval_crf(x,y)\n",
    "crf = train_crf(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Format Unannotated Sentances for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentances(file):\n",
    "    with open(file) as f:\n",
    "            lines = [i.rstrip().split(\" \") for i in f.readlines()]\n",
    "    return lines\n",
    "\n",
    "def add_pos_sentances(seqs):\n",
    "    new_seqs = []\n",
    "    for sentance in seqs:\n",
    "        pos = pos_tag(sentance)        \n",
    "        new_seqs.append(pos)\n",
    "    return new_seqs\n",
    "\n",
    "def remove_null_words(seq):\n",
    "    return [word for word in seq if len(word)>0]\n",
    "\n",
    "def prep_unannotated_data(file):\n",
    "    # Load Data \n",
    "    seqs = load_sentances(file)\n",
    "    \n",
    "    # Remove null words\n",
    "    seqs = [remove_null_words(seq) for seq in seqs]\n",
    "    \n",
    "    # Add CRF Features\n",
    "    seqs = add_pos_sentances(seqs)\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('on', 'IN'),\n",
       " ('home', 'NN'),\n",
       " ('O2', 'NNP'),\n",
       " (',', ','),\n",
       " ('who', 'WP'),\n",
       " ('presents', 'VBZ'),\n",
       " ('with', 'IN'),\n",
       " ('three', 'CD'),\n",
       " ('days', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('shortness', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('breath', 'NN'),\n",
       " ('thought', 'VBN'),\n",
       " ('by', 'IN')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unannot_file = \"./data/sentances_0-10.txt\"\n",
    "seqs = prep_unannotated_data(unannot_file)\n",
    "seqs[0][15:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Annotations\n",
    "\n",
    "Data is ready to be used by CRF, tags will needed to be formatted post prediciton in order to capture multi word entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annotations(crf,seqs):\n",
    "    x = [sent2features(s) for s in seqs]\n",
    "    ner_tags = crf.predict(x)\n",
    "    return ner_tags\n",
    "\n",
    "def combine_B_I_tags(seq):\n",
    "    combi_seq = []\n",
    "    phrase = \"\"\n",
    "    for word,tag in seq:\n",
    "        if tag == \"O\":\n",
    "            if len(phrase)>0:\n",
    "                combi_seq.append([phrase,phrase_tag])\n",
    "                phrase = \"\"\n",
    "            combi_seq.append([word,tag])\n",
    "        else:\n",
    "            if tag[0] == \"B\":\n",
    "                if len(phrase)>0:\n",
    "                    combi_seq.append([phrase,phrase_tag])\n",
    "                    phrase = \"\"\n",
    "                phrase = word\n",
    "                phrase_tag = tag[2:]\n",
    "            if tag[0] == \"I\":\n",
    "                phrase += \" \"+word\n",
    "    if len(phrase)>0:\n",
    "        combi_seq.append([phrase,phrase_tag])\n",
    "    return combi_seq\n",
    "\n",
    "def create_dataset(crf,seqs):\n",
    "    ner_tags = create_annotations(crf,seqs)\n",
    "    data = []\n",
    "    for i in range(len(seqs)):\n",
    "        data_entry = [[seqs[i][j][0],ner_tags[i][j]] for j in range(len(seqs[i]))]\n",
    "        data.append(data_entry)\n",
    "    data = [combine_B_I_tags(seq) for seq in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['on', 'O'],\n",
       " ['home O2', 'Drug'],\n",
       " [',', 'O'],\n",
       " ['who', 'O'],\n",
       " ['presents', 'O'],\n",
       " ['with', 'O'],\n",
       " ['three days', 'Duration'],\n",
       " ['of', 'O'],\n",
       " ['shortness of breath', 'DOS'],\n",
       " ['thought', 'O']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = create_dataset(crf,seqs)\n",
    "data[0][15:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Format Annotations for DataTurks Ingestion:\n",
    "\n",
    "Each line in .txt file will be in the following format:\n",
    "\n",
    "{\"content\":\"cd players and tuners\",\"annotation\":[{\"label\":[\"Category\"],\"points\":[{\"start\":0,\"end\":1,\"text\":\"cd\"}]},{\"label\":[\"Category\"],\"points\":[{\"start\":3,\"end\":9,\"text\":\"players\"}]},{\"label\":[\"Category\"],\"points\":[{\"start\":15,\"end\":20,\"text\":\"tuners\"}]}],\"extras\":{\"Name\":\"columnName\",\"Class\":\"ColumnValue\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points(seq):\n",
    "    start = 0\n",
    "    end = 0\n",
    "    starts = []\n",
    "    ends = []\n",
    "    for entry in seq:\n",
    "        starts.append(start)\n",
    "        start += len(entry[0])+1\n",
    "        ends.append(start-2)\n",
    "    new_seq = [[seq[i][0],seq[i][1],starts[i],ends[i]] for i in range(len(seq))]\n",
    "    return new_seq\n",
    "\n",
    "def get_annotation(seq,ignore_ents):\n",
    "    new_seq = get_points(seq)\n",
    "    annotations = []\n",
    "    for text,tag,start,end in new_seq:\n",
    "        if tag not in ignore_ents:\n",
    "            annot = {}\n",
    "            annot[\"label\"] = [tag]\n",
    "            annot[\"points\"] = [{\"start\":start,\"end\":end,\"text\":text}]\n",
    "            annotations.append(annot)\n",
    "    return annotations\n",
    "\n",
    "def write_json(file,data,ignore_tags):\n",
    "    with open(file,\"w\") as f:\n",
    "        for seq in data:\n",
    "            line = {}\n",
    "            line[\"content\"] = \" \".join([i[0] for i in seq])\n",
    "            line[\"annotation\"]  = get_annotation(seq,ignore_tags)\n",
    "            line[\"extras\"] = {\"Name\":\"ColumnName\",\"Class\":\"ColumnValue\"}\n",
    "            f.write(json.dumps(line))\n",
    "            f.write(\"\\n\")\n",
    "    print(\"~~~Annotations Saved~~~\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~Annotations Saved~~~\n"
     ]
    }
   ],
   "source": [
    "# Cutoff Precision of 0.6 when evaluating Tags\n",
    "ignore_tags = [\"O\",\n",
    "               \"Duration\",\n",
    "               \"BODY\"]\n",
    "\n",
    "save_file = \"./data/pre_annot_sentances_0-10.txt\"\n",
    "\n",
    "write_json(save_file,data,ignore_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def smart_predict(annotated_file,unannotated_file,save_file,ignore_tags=[\"O\"],eval_only=False):\n",
    "    '''\n",
    "    Inputs:\n",
    "    \n",
    "    annotated_file - file path to labelled data in .tsv format outputed from dataturks\n",
    "    \n",
    "    unannotated_file - file path to .txt file with each line containing an unannotated sentance\n",
    "    \n",
    "    save_file - file path to annotations generatated by crf model for upload to dataturks\n",
    "    \n",
    "    eval_only - if set to True will print a classification report using 10% of the annotated data as\n",
    "                a test set. Can be used to determine which tags to ignore.\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    None\n",
    "    \n",
    "    Desc:\n",
    "    \n",
    "    Loads annotated data, trains crf model. Makes predictions on unannottated sentances and saves\n",
    "    output for upload to dataturks annotation service. \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    x,y,_ = create_crf_data(annotated_file)\n",
    "    \n",
    "    if eval_only:\n",
    "        eval_crf(x,y)\n",
    "        return\n",
    "    \n",
    "    crf = train_crf(x,y)\n",
    "\n",
    "    seqs = prep_unannotated_data(unannotated_file)\n",
    "    data = create_dataset(crf,seqs)\n",
    "\n",
    "    write_json(save_file,data,ignore_tags)\n",
    "    return\n",
    "\n",
    "def show_crf_eval(annotated_file):\n",
    "    x,y,tokens = create_crf_data(annotated_file)\n",
    "    if show_eval:\n",
    "        eval_crf(x,y)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      " ------------------------------------------------------------\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "                       DOS       0.69      0.66      0.67       357\n",
      "                 Procedure       0.66      0.57      0.61        91\n",
      "        Patient Relocation       0.81      0.81      0.81        83\n",
      "                      Date       0.85      0.81      0.83        42\n",
      "                      BODY       0.47      0.46      0.47        56\n",
      "Other Measurement / Result       0.73      0.63      0.68        81\n",
      "                      Drug       0.91      0.86      0.88       146\n",
      "                     Route       0.90      0.86      0.88        51\n",
      "                       GEO       0.87      0.86      0.86       118\n",
      "                      Time       0.66      0.70      0.68        67\n",
      "                 Condition       0.66      0.65      0.65       138\n",
      "               Temperature       0.82      0.69      0.75        13\n",
      "                  Duration       0.37      0.30      0.33        23\n",
      "                       POI       0.75      0.58      0.65        31\n",
      "          Respiratory Rate       1.00      0.80      0.89        10\n",
      "                 Frequency       0.78      0.44      0.56        16\n",
      "                       Age       1.00      1.00      1.00        20\n",
      "          Test / Screening       0.84      0.76      0.80        71\n",
      "             O2 Saturation       1.00      0.77      0.87        30\n",
      "          Accident / Event       0.64      0.50      0.56        14\n",
      "                Heart Rate       0.71      0.62      0.67        16\n",
      "                      Dose       0.97      0.94      0.95        65\n",
      "            Blood Pressure       0.78      0.82      0.80        22\n",
      "                  Quantity       0.73      0.50      0.59        22\n",
      "                    Gender       1.00      0.93      0.97        15\n",
      "\n",
      "                 micro avg       0.76      0.71      0.74      1598\n",
      "                 macro avg       0.76      0.71      0.74      1598\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "annotated_file = \"./data/Medical NER V2 4000.tsv\"\n",
    "unannot_file = \"./data/sentances_0-10.txt\"\n",
    "save_file = \"./data/pre_annot_sentances_0-10.txt\"\n",
    "\n",
    "smart_predict(annotated_file,unannot_file,save_file,eval_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is the most important for annotations we want to make sure that the highest percentage of annotations are relevant. If the model misses an annotation its no difference than if it was unannotated before but if it gives us a false positive we will have to delete it which will cost time.\n",
    "\n",
    "Here for example if we set a precision cut off of 0.6 we will ignore the \"Duration\" and \"BODY\" tags as we will likely have to delete >40% of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~Annotations Saved~~~\n"
     ]
    }
   ],
   "source": [
    "ignore_tags = [\"O\",\n",
    "               \"Duration\",\n",
    "               \"BODY\",\n",
    "              ]\n",
    "\n",
    "smart_predict(annotated_file,unannot_file,save_file,ignore_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras36]",
   "language": "python",
   "name": "conda-env-keras36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
