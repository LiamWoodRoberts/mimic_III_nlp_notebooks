{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation Pipeline for DataTurks .tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from seqeval.metrics import f1_score,classification_report\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional,Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Data to Sequences\n",
    "\n",
    "### Contents:\n",
    "\n",
    "1. Reading File and formatting as sequences\n",
    "2. Formatting Entities to IOB Scheme\n",
    "3. Padding Sequences\n",
    "4. Mapping to Integer Ids\n",
    "5. Formatting Data for Keras LSTM Model\n",
    "6. Train Test Split\n",
    "7. Specifying Model and Model Parameters\n",
    "8. Training Model\n",
    "9. Evaluating Model\n",
    "\n",
    "### 1. Reading and Formatting File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_turks(file):\n",
    "    with open(file) as f:\n",
    "        lines = [i.rstrip().split(\"\\t\") for i in f.readlines()]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0,\"HISTORY', 'O'],\n",
       " ['OF', 'O'],\n",
       " ['PRESENT', 'O'],\n",
       " ['ILLNESS', 'O'],\n",
       " ['This', 'O'],\n",
       " ['is', 'O'],\n",
       " ['an', 'O'],\n",
       " ['81-year-old', 'CONDITION/SYMPTOM'],\n",
       " ['female', 'CONDITION/SYMPTOM'],\n",
       " ['with', 'O'],\n",
       " ['a', 'O'],\n",
       " ['history', 'O'],\n",
       " ['of', 'O'],\n",
       " ['emphysema', 'CONDITION/SYMPTOM'],\n",
       " ['not', 'O'],\n",
       " ['on', 'O'],\n",
       " ['home', 'DRUG'],\n",
       " ['O2', 'DRUG'],\n",
       " [',', 'O'],\n",
       " ['who', 'O']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"./nlp_data/Medical NER Dataset 2600.tsv\"\n",
    "word_ents = read_turks(file)\n",
    "word_ents[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(word_ents):\n",
    "    '''removes quote and comma characters from '''\n",
    "    new_word_ents = []\n",
    "    for ents in word_ents:\n",
    "        word = ents[0].lower()\n",
    "        if word.find(',') > 0:\n",
    "            word = word[word.find(',')+1:]\n",
    "        word = word.replace('\"','')\n",
    "        ents[0] = word\n",
    "        new_word_ents.append(ents)\n",
    "    return new_word_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['history', 'O'],\n",
       " ['of', 'O'],\n",
       " ['present', 'O'],\n",
       " ['illness', 'O'],\n",
       " ['this', 'O'],\n",
       " ['is', 'O'],\n",
       " ['an', 'O'],\n",
       " ['81-year-old', 'CONDITION/SYMPTOM'],\n",
       " ['female', 'CONDITION/SYMPTOM'],\n",
       " ['with', 'O'],\n",
       " ['a', 'O'],\n",
       " ['history', 'O'],\n",
       " ['of', 'O'],\n",
       " ['emphysema', 'CONDITION/SYMPTOM'],\n",
       " ['not', 'O'],\n",
       " ['on', 'O'],\n",
       " ['home', 'DRUG'],\n",
       " ['o2', 'DRUG'],\n",
       " [',', 'O'],\n",
       " ['who', 'O']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ents = clean_words(word_ents)\n",
    "new_ents[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seqs(word_ents):\n",
    "    seqs = []\n",
    "    seq = []\n",
    "    for ents in word_ents:\n",
    "        if len(ents)>1:\n",
    "            if len(ents[0])>0:\n",
    "                seq.append(ents)\n",
    "        else:\n",
    "            seqs.append(seq)\n",
    "            seq=[]\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['history', 'O'],\n",
       "  ['of', 'O'],\n",
       "  ['present', 'O'],\n",
       "  ['illness', 'O'],\n",
       "  ['this', 'O'],\n",
       "  ['is', 'O'],\n",
       "  ['an', 'O'],\n",
       "  ['81-year-old', 'CONDITION/SYMPTOM'],\n",
       "  ['female', 'CONDITION/SYMPTOM'],\n",
       "  ['with', 'O'],\n",
       "  ['a', 'O'],\n",
       "  ['history', 'O'],\n",
       "  ['of', 'O'],\n",
       "  ['emphysema', 'CONDITION/SYMPTOM'],\n",
       "  ['not', 'O'],\n",
       "  ['on', 'O'],\n",
       "  ['home', 'DRUG'],\n",
       "  ['o2', 'DRUG'],\n",
       "  [',', 'O'],\n",
       "  ['who', 'O'],\n",
       "  ['presents', 'O'],\n",
       "  ['with', 'O'],\n",
       "  ['three', 'AMOUNT'],\n",
       "  ['days', 'AMOUNT'],\n",
       "  ['of', 'O'],\n",
       "  ['shortness', 'CONDITION/SYMPTOM'],\n",
       "  ['of', 'CONDITION/SYMPTOM'],\n",
       "  ['breath', 'CONDITION/SYMPTOM'],\n",
       "  ['thought', 'O'],\n",
       "  ['by', 'O'],\n",
       "  ['her', 'O'],\n",
       "  ['primary', 'O'],\n",
       "  ['care', 'O'],\n",
       "  ['doctor', 'O'],\n",
       "  ['to', 'O'],\n",
       "  ['be', 'O'],\n",
       "  ['a', 'O'],\n",
       "  ['copd', 'CONDITION/SYMPTOM'],\n",
       "  ['flare', 'CONDITION/SYMPTOM']]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = create_seqs(new_ents)\n",
    "seqs[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we add an extra step to split on special characters to reduce the number of OOV words for our Glove Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_word(ent):\n",
    "    words = re.split('(\\W)',ent[0])\n",
    "    return [[i,ent[1]] for i in words ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_special_chars(seqs):\n",
    "    new_seqs = []\n",
    "    for seq in seqs:\n",
    "        new_seq = []\n",
    "        for word in seq:\n",
    "            new_seq += expand_word(word)\n",
    "        new_seqs.append(new_seq)\n",
    "    return new_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['history', 'O'],\n",
       " ['of', 'O'],\n",
       " ['present', 'O'],\n",
       " ['illness', 'O'],\n",
       " ['this', 'O'],\n",
       " ['is', 'O'],\n",
       " ['an', 'O'],\n",
       " ['81', 'CONDITION/SYMPTOM'],\n",
       " ['-', 'CONDITION/SYMPTOM'],\n",
       " ['year', 'CONDITION/SYMPTOM'],\n",
       " ['-', 'CONDITION/SYMPTOM'],\n",
       " ['old', 'CONDITION/SYMPTOM'],\n",
       " ['female', 'CONDITION/SYMPTOM'],\n",
       " ['with', 'O'],\n",
       " ['a', 'O'],\n",
       " ['history', 'O'],\n",
       " ['of', 'O'],\n",
       " ['emphysema', 'CONDITION/SYMPTOM'],\n",
       " ['not', 'O'],\n",
       " ['on', 'O'],\n",
       " ['home', 'DRUG'],\n",
       " ['o2', 'DRUG'],\n",
       " ['', 'O'],\n",
       " [',', 'O'],\n",
       " ['', 'O'],\n",
       " ['who', 'O'],\n",
       " ['presents', 'O'],\n",
       " ['with', 'O'],\n",
       " ['three', 'AMOUNT'],\n",
       " ['days', 'AMOUNT'],\n",
       " ['of', 'O'],\n",
       " ['shortness', 'CONDITION/SYMPTOM'],\n",
       " ['of', 'CONDITION/SYMPTOM'],\n",
       " ['breath', 'CONDITION/SYMPTOM'],\n",
       " ['thought', 'O'],\n",
       " ['by', 'O'],\n",
       " ['her', 'O'],\n",
       " ['primary', 'O'],\n",
       " ['care', 'O'],\n",
       " ['doctor', 'O'],\n",
       " ['to', 'O'],\n",
       " ['be', 'O'],\n",
       " ['a', 'O'],\n",
       " ['copd', 'CONDITION/SYMPTOM'],\n",
       " ['flare', 'CONDITION/SYMPTOM']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_seqs = expand_special_chars(seqs)\n",
    "ex_seqs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Formatting Entities to IOB (Inside,Outside, Beginning) Scheme "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tags(word_ents):\n",
    "    '''adds IOB scheme to tags'''\n",
    "    new_ents = []\n",
    "    for i in range(0,len(word_ents)):\n",
    "        if word_ents[i][1] == \"O\":\n",
    "            tag = word_ents[i][1]\n",
    "        else:\n",
    "            if not i:\n",
    "                tag = \"B-\"+word_ents[i][1]\n",
    "            else:\n",
    "                if (word_ents[i][1] != word_ents[i-1][1]):\n",
    "                    tag = \"B-\"+word_ents[i][1]\n",
    "                else:\n",
    "                    tag = \"I-\"+word_ents[i][1]\n",
    "\n",
    "        new_ents.append([word_ents[i][0],tag])\n",
    "    return new_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['history', 'O'],\n",
       " ['of', 'O'],\n",
       " ['present', 'O'],\n",
       " ['illness', 'O'],\n",
       " ['this', 'O'],\n",
       " ['is', 'O'],\n",
       " ['an', 'O'],\n",
       " ['81', 'B-CONDITION/SYMPTOM'],\n",
       " ['-', 'I-CONDITION/SYMPTOM'],\n",
       " ['year', 'I-CONDITION/SYMPTOM'],\n",
       " ['-', 'I-CONDITION/SYMPTOM'],\n",
       " ['old', 'I-CONDITION/SYMPTOM'],\n",
       " ['female', 'I-CONDITION/SYMPTOM'],\n",
       " ['with', 'O'],\n",
       " ['a', 'O'],\n",
       " ['history', 'O'],\n",
       " ['of', 'O'],\n",
       " ['emphysema', 'B-CONDITION/SYMPTOM'],\n",
       " ['not', 'O'],\n",
       " ['on', 'O'],\n",
       " ['home', 'B-DRUG'],\n",
       " ['o2', 'I-DRUG'],\n",
       " ['', 'O'],\n",
       " [',', 'O'],\n",
       " ['', 'O'],\n",
       " ['who', 'O'],\n",
       " ['presents', 'O'],\n",
       " ['with', 'O'],\n",
       " ['three', 'B-AMOUNT'],\n",
       " ['days', 'I-AMOUNT'],\n",
       " ['of', 'O'],\n",
       " ['shortness', 'B-CONDITION/SYMPTOM'],\n",
       " ['of', 'I-CONDITION/SYMPTOM'],\n",
       " ['breath', 'I-CONDITION/SYMPTOM'],\n",
       " ['thought', 'O'],\n",
       " ['by', 'O'],\n",
       " ['her', 'O'],\n",
       " ['primary', 'O'],\n",
       " ['care', 'O'],\n",
       " ['doctor', 'O'],\n",
       " ['to', 'O'],\n",
       " ['be', 'O'],\n",
       " ['a', 'O'],\n",
       " ['copd', 'B-CONDITION/SYMPTOM'],\n",
       " ['flare', 'I-CONDITION/SYMPTOM']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tag_seqs = [clean_tags(ents) for ents in ex_seqs]\n",
    "cleaned_tag_seqs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Padding Sequences to a Specified Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(seq,max_len):\n",
    "    padded_seq = seq+[[\"<PAD>\",\"O\"]]*max_len\n",
    "    return padded_seq[:max_len]\n",
    "    \n",
    "def pad_sequences(sequences,max_len=None):\n",
    "    if max_len == None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "    return [pad_seq(seq,max_len) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['history', 'O'],\n",
       " ['of', 'O'],\n",
       " ['present', 'O'],\n",
       " ['illness', 'O'],\n",
       " ['this', 'O'],\n",
       " ['is', 'O'],\n",
       " ['an', 'O'],\n",
       " ['81', 'B-CONDITION/SYMPTOM'],\n",
       " ['-', 'I-CONDITION/SYMPTOM'],\n",
       " ['year', 'I-CONDITION/SYMPTOM'],\n",
       " ['-', 'I-CONDITION/SYMPTOM'],\n",
       " ['old', 'I-CONDITION/SYMPTOM'],\n",
       " ['female', 'I-CONDITION/SYMPTOM'],\n",
       " ['with', 'O'],\n",
       " ['a', 'O'],\n",
       " ['history', 'O'],\n",
       " ['of', 'O'],\n",
       " ['emphysema', 'B-CONDITION/SYMPTOM'],\n",
       " ['not', 'O'],\n",
       " ['on', 'O'],\n",
       " ['home', 'B-DRUG'],\n",
       " ['o2', 'I-DRUG'],\n",
       " ['', 'O'],\n",
       " [',', 'O'],\n",
       " ['', 'O'],\n",
       " ['who', 'O'],\n",
       " ['presents', 'O'],\n",
       " ['with', 'O'],\n",
       " ['three', 'B-AMOUNT'],\n",
       " ['days', 'I-AMOUNT'],\n",
       " ['of', 'O'],\n",
       " ['shortness', 'B-CONDITION/SYMPTOM'],\n",
       " ['of', 'I-CONDITION/SYMPTOM'],\n",
       " ['breath', 'I-CONDITION/SYMPTOM'],\n",
       " ['thought', 'O'],\n",
       " ['by', 'O'],\n",
       " ['her', 'O'],\n",
       " ['primary', 'O'],\n",
       " ['care', 'O'],\n",
       " ['doctor', 'O'],\n",
       " ['to', 'O'],\n",
       " ['be', 'O'],\n",
       " ['a', 'O'],\n",
       " ['copd', 'B-CONDITION/SYMPTOM'],\n",
       " ['flare', 'I-CONDITION/SYMPTOM'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O'],\n",
       " ['<PAD>', 'O']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 50\n",
    "padded_seqs = pad_sequences(cleaned_tag_seqs,max_len)\n",
    "padded_seqs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Mapping Words to Integer Values for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ids(sentances,tag=False):\n",
    "    words = []\n",
    "    for sentance in sentances:\n",
    "        words += list([word[tag] for word in sentance])\n",
    "    word_dict = {word:i for i,word in enumerate(set(words))}\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 0), ('here', 1), ('10', 2), ('hospital6', 3)]\n",
      "[('I-DATE', 0), ('B-DRUG', 1), ('B-GENDER', 2), ('I-LOCATION', 3)]\n"
     ]
    }
   ],
   "source": [
    "word_ids = get_word_ids(padded_seqs)\n",
    "tag_ids = get_word_ids(padded_seqs,tag=True)\n",
    "print(list(word_ids.items())[:4])\n",
    "print(list(tag_ids.items())[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_ids(sentances,word_ids,tag_ids):\n",
    "    vector = []\n",
    "    for sentance in sentances:\n",
    "        vector.append(list([[word_ids[w[0]],tag_ids[w[1]]] for w in sentance]))\n",
    "    return np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3445   12]\n",
      " [2251   12]\n",
      " [1680   12]\n",
      " [2038   12]]\n",
      "\n",
      "Word Representation:\n",
      "[['history', 'O'], ['of', 'O'], ['present', 'O'], ['illness', 'O']]\n"
     ]
    }
   ],
   "source": [
    "vectors = words_to_ids(padded_seqs,word_ids,tag_ids)\n",
    "print(vectors[0][:4])\n",
    "print('')\n",
    "print(\"Word Representation:\")\n",
    "print(padded_seqs[0][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x_y(matrix,n_tags):\n",
    "    x = []\n",
    "    y = []\n",
    "    for sequences in matrix:\n",
    "        xi = [i[0] for i in sequences]\n",
    "        yi = [i[1] for i in sequences]\n",
    "        x.append(xi)\n",
    "        y.append(yi)\n",
    "    y = np.array([to_categorical(i,n_tags) for i in y])\n",
    "    return np.array(x),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-shape: (2592, 50)\n",
      "[3445 2251 1680 2038 4491]\n",
      "\n",
      "Y-shape: (2592, 50, 24)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "n_tags = len(tag_ids)\n",
    "x,y = create_x_y(vectors,n_tags)\n",
    "print(\"X-shape:\",x.shape)\n",
    "print(x[0][:5])\n",
    "print('')\n",
    "print(\"Y-shape:\",y.shape)\n",
    "print(y[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Specifying Model and Model Parameters\n",
    "\n",
    "https://www.depends-on-the-definition.com/introduction-named-entity-recognition-python/\n",
    "\n",
    "#### Use Pre-Trained Glove Word Embeddings\n",
    "\n",
    "GLOVE Code:\n",
    "https://medium.com/@sabber/classifying-yelp-review-comments-using-cnn-lstm-and-pre-trained-glove-word-embeddings-part-3-53fcea9a17fa\n",
    "\n",
    "Embeddings Available:\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filepath):\n",
    "    with open(f\"{filepath}glove.6B.100d.txt\") as f:\n",
    "        embeddings_index={}\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.array(values[1:],dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "def create_weight_matrix(word_ids,embeddings_index):\n",
    "    embedding_matrix = np.zeros((len(word_ids),100))\n",
    "    count = 0\n",
    "    oov_words = []\n",
    "    for word,idx in word_ids.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "        else:\n",
    "            oov_words.append(word)\n",
    "    return embedding_matrix,oov_words    \n",
    "\n",
    "glove_path = \"./glove.6B/\"\n",
    "glove_emb = load_embeddings(glove_path)\n",
    "embed_matrix,oov_words = create_weight_matrix(word_ids,glove_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent OOV: 13.945729537366546%\n",
      "['', 'hospital6', 'transluminal', 'hypernatremic', 'precipitant', 't99', 'stented', 'pneumoperitoneum', 'fsgs', 'rhabdo', 'endorces', 'esophagogastroduodenoscopy', 'dyuria', 'azitrhomycin', 'cefepine', 'recived', 'neseritide', 'nonhodgkin', 'hypervolemia', 'hypersensitivty', 'bolused', 'dysuria', '1hour', 'atrius', 'stridor', 'valvuloplasty', 'transesophageal', '68yo', '2173', 'apneas']\n"
     ]
    }
   ],
   "source": [
    "# OOV Words\n",
    "print(f\"Percent OOV: {len(oov_words)/len(word_ids)*100}%\")\n",
    "print(oov_words[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 14% of the words are OOV. This high number does make some sense as so much of the medical text contains highly specialized words such as drug names, measurements and abreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_BiLSTM(n_words,n_tags,embedding_size,max_len,embed_weights):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(n_words,\n",
    "                        embedding_size,\n",
    "                        weights=[embed_weights],\n",
    "                        trainable=False,\n",
    "                        input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.2)))\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"softmax\")))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 100)           449600    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 50, 200)           160800    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 50, 24)            4824      \n",
      "=================================================================\n",
      "Total params: 615,224\n",
      "Trainable params: 165,624\n",
      "Non-trainable params: 449,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 100\n",
    "n_words = len(word_ids)\n",
    "n_tags = len(tag_ids)\n",
    "\n",
    "model = create_BiLSTM(n_words,n_tags,embedding_size,max_len,embed_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model(model,x_train,y_train,batch_size=32,epochs=20,val_split = 0.1):\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0.0001,\n",
    "                               patience=3,\n",
    "                               mode='min',\n",
    "                               verbose=1)\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit(x_train, y_train, \n",
    "                        batch_size=32, \n",
    "                        epochs=epochs, \n",
    "                        validation_split=val_split, \n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stop]\n",
    "                       )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2098 samples, validate on 234 samples\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From /Users/LiamRoberts/anaconda3/envs/keras36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1a3a9612f0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1a3a9612f0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "2098/2098 [==============================] - 14s 7ms/sample - loss: 0.7491 - accuracy: 0.8435 - val_loss: 0.5282 - val_accuracy: 0.8569\n",
      "Epoch 2/50\n",
      "2098/2098 [==============================] - 11s 5ms/sample - loss: 0.4693 - accuracy: 0.8697 - val_loss: 0.4281 - val_accuracy: 0.8741\n",
      "Epoch 3/50\n",
      "2098/2098 [==============================] - 12s 6ms/sample - loss: 0.3994 - accuracy: 0.8833 - val_loss: 0.3841 - val_accuracy: 0.8856\n",
      "Epoch 4/50\n",
      "2098/2098 [==============================] - 11s 5ms/sample - loss: 0.3577 - accuracy: 0.8936 - val_loss: 0.3479 - val_accuracy: 0.8967\n",
      "Epoch 5/50\n",
      "2098/2098 [==============================] - 11s 5ms/sample - loss: 0.3311 - accuracy: 0.9000 - val_loss: 0.3237 - val_accuracy: 0.9037\n",
      "Epoch 6/50\n",
      "2098/2098 [==============================] - 10s 5ms/sample - loss: 0.3065 - accuracy: 0.9063 - val_loss: 0.3139 - val_accuracy: 0.9032\n",
      "Epoch 7/50\n",
      "2098/2098 [==============================] - 11s 5ms/sample - loss: 0.2888 - accuracy: 0.9101 - val_loss: 0.3080 - val_accuracy: 0.9051\n",
      "Epoch 8/50\n",
      "2098/2098 [==============================] - 11s 5ms/sample - loss: 0.2753 - accuracy: 0.9141 - val_loss: 0.2996 - val_accuracy: 0.9066\n",
      "Epoch 9/50\n",
      "2098/2098 [==============================] - 10s 5ms/sample - loss: 0.2624 - accuracy: 0.9181 - val_loss: 0.2810 - val_accuracy: 0.9118\n",
      "Epoch 10/50\n",
      "2098/2098 [==============================] - 10s 5ms/sample - loss: 0.2499 - accuracy: 0.9212 - val_loss: 0.2671 - val_accuracy: 0.9175\n",
      "Epoch 11/50\n",
      "2098/2098 [==============================] - 10s 5ms/sample - loss: 0.2402 - accuracy: 0.9239 - val_loss: 0.2777 - val_accuracy: 0.9131\n",
      "Epoch 12/50\n",
      "2098/2098 [==============================] - 10s 5ms/sample - loss: 0.2329 - accuracy: 0.9264 - val_loss: 0.2637 - val_accuracy: 0.9176\n",
      "Epoch 13/50\n",
      "2098/2098 [==============================] - 9s 4ms/sample - loss: 0.2230 - accuracy: 0.9287 - val_loss: 0.2520 - val_accuracy: 0.9218\n",
      "Epoch 14/50\n",
      "2098/2098 [==============================] - 9s 4ms/sample - loss: 0.2156 - accuracy: 0.9312 - val_loss: 0.2563 - val_accuracy: 0.9200\n",
      "Epoch 15/50\n",
      "2098/2098 [==============================] - 9s 4ms/sample - loss: 0.2110 - accuracy: 0.9328 - val_loss: 0.2532 - val_accuracy: 0.9227\n",
      "Epoch 16/50\n",
      "2098/2098 [==============================] - 9s 4ms/sample - loss: 0.2025 - accuracy: 0.9352 - val_loss: 0.2404 - val_accuracy: 0.9252\n",
      "Epoch 17/50\n",
      "2098/2098 [==============================] - 9s 4ms/sample - loss: 0.1965 - accuracy: 0.9366 - val_loss: 0.2465 - val_accuracy: 0.9219\n",
      "Epoch 18/50\n",
      "2098/2098 [==============================] - 10s 5ms/sample - loss: 0.1909 - accuracy: 0.9389 - val_loss: 0.2427 - val_accuracy: 0.9263\n",
      "Epoch 19/50\n",
      "2080/2098 [============================>.] - ETA: 0s - loss: 0.1873 - accuracy: 0.9396"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 32\n",
    "history = train_model(model,x_train,y_train,batch_size=batch_size,epochs=epochs,val_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_mappings(ids):\n",
    "    return {str(i[1]):i[0] for i in ids.items()}\n",
    "\n",
    "def generate_sample(x,y,model):\n",
    "    idx = random.randint(0,len(x))\n",
    "    sample = x[idx]\n",
    "    label = np.argmax(y[idx],axis=1)\n",
    "\n",
    "    p = model.predict(sample.reshape(1,-1))\n",
    "    p = np.argmax(p,axis=-1)\n",
    "    print(\"{:25} {:20}: {:10}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "    print(\"-\"*50)\n",
    "    for i in range(len(sample)):\n",
    "        word = str(sample[i])\n",
    "        pred = str(p[0][i])\n",
    "        true_val = str(label[i])\n",
    "        id_to_words = get_id_mappings(word_ids)\n",
    "        id_to_tags = get_id_mappings(tag_ids)\n",
    "        print(f\"{id_to_words[word]:25}{id_to_tags[true_val]:20}{id_to_tags[pred]}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sample(x,y,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_ids_to_tags(preds,tag_ids):\n",
    "    id_to_tags = get_id_mappings(tag_ids)\n",
    "\n",
    "    tag_seqs = []\n",
    "    for seq in preds:\n",
    "        tag_seqs.append([id_to_tags[str(i)] for i in seq])\n",
    "    return tag_seqs\n",
    "\n",
    "def get_real_labels(model,x_test,y_test,tag_ids):\n",
    "    test_preds = np.argmax(model.predict(x_test),axis=-1)\n",
    "    true_vals = np.argmax(y_test,axis=-1)\n",
    "    test_preds = transform_ids_to_tags(test_preds,tag_ids)\n",
    "    true_vals = transform_ids_to_tags(true_vals,tag_ids)\n",
    "    return true_vals,test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vals,test_preds = get_real_labels(model,x_test,y_test,tag_ids)\n",
    "report = classification_report(true_vals,test_preds)\n",
    "print(f1_score(true_vals,test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f1 = f1_score(true_vals,test_preds)\n",
    "\n",
    "n_samples = len(x)\n",
    "model_desc = f\"BiLSTM-Glove-EmbedSize-{embedding_size}\"\n",
    "results_file = \"./nlp_data/model_results.csv\"\n",
    "note = '''Max Len Reverted Back to 50 Words'''\n",
    "def append_model_results(model_f1,n_samples,model_desc,file,note):\n",
    "    with open(file,'a') as f:\n",
    "        results = f\"\\n{model_f1},{n_samples},{model_desc},{time.ctime()},{note}\"\n",
    "        f.writelines(results)\n",
    "    print(\"~~~Results Successfully Saved\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_model_results(model_f1,n_samples,model_desc,results_file,note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(results_file).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras36]",
   "language": "python",
   "name": "conda-env-keras36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
